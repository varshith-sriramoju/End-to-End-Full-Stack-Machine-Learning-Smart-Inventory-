{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartInventory - Demand Forecasting Exploration\n",
    "\n",
    "This notebook explores the sales data and develops the demand forecasting model for SmartInventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "df = pd.read_csv('../sample_sales_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check date range\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Number of unique stores: {df['store_id'].nunique()}\")\n",
    "print(f\"Number of unique products: {df['sku_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sales histogram\n",
    "axes[0, 0].hist(df['sales'], bins=50, alpha=0.7)\n",
    "axes[0, 0].set_title('Sales Distribution')\n",
    "axes[0, 0].set_xlabel('Sales')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Price histogram\n",
    "axes[0, 1].hist(df['price'], bins=50, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Price Distribution')\n",
    "axes[0, 1].set_xlabel('Price')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Inventory histogram\n",
    "axes[1, 0].hist(df['on_hand'], bins=50, alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Inventory Distribution')\n",
    "axes[1, 0].set_xlabel('On Hand')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Promotions pie chart\n",
    "promo_counts = df['promotions_flag'].value_counts()\n",
    "axes[1, 1].pie(promo_counts.values, labels=['No Promotion', 'Promotion'], autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Promotion Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "daily_sales = df.groupby('date')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Total daily sales\n",
    "axes[0].plot(daily_sales['date'], daily_sales['sum'])\n",
    "axes[0].set_title('Total Daily Sales')\n",
    "axes[0].set_ylabel('Total Sales')\n",
    "\n",
    "# Average daily sales\n",
    "axes[1].plot(daily_sales['date'], daily_sales['mean'], color='orange')\n",
    "axes[1].set_title('Average Daily Sales per Transaction')\n",
    "axes[1].set_ylabel('Average Sales')\n",
    "\n",
    "# Number of transactions\n",
    "axes[2].plot(daily_sales['date'], daily_sales['count'], color='green')\n",
    "axes[2].set_title('Number of Daily Transactions')\n",
    "axes[2].set_ylabel('Transaction Count')\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal patterns\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Day of week pattern\n",
    "dow_sales = df.groupby('day_of_week')['sales'].mean()\n",
    "dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0].bar(range(7), dow_sales.values)\n",
    "axes[0].set_xticks(range(7))\n",
    "axes[0].set_xticklabels(dow_labels)\n",
    "axes[0].set_title('Average Sales by Day of Week')\n",
    "axes[0].set_ylabel('Average Sales')\n",
    "\n",
    "# Monthly pattern\n",
    "monthly_sales = df.groupby('month')['sales'].mean()\n",
    "axes[1].bar(monthly_sales.index, monthly_sales.values, color='orange')\n",
    "axes[1].set_title('Average Sales by Month')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Average Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for modeling\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['store_id', 'sku_id', 'date'])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in ['store_id', 'sku_id']:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = label_encoders[col].fit_transform(df[col])\n",
    "    \n",
    "    # Time-based features\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 7, 14, 30]:\n",
    "        df[f'sales_lag_{lag}'] = df.groupby(['store_id', 'sku_id'])['sales'].shift(lag)\n",
    "    \n",
    "    # Rolling averages\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'sales_rolling_{window}'] = df.groupby(['store_id', 'sku_id'])['sales'].rolling(window=window).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Price features\n",
    "    df['price_change'] = df.groupby(['store_id', 'sku_id'])['price'].pct_change()\n",
    "    df['price_rolling_7'] = df.groupby(['store_id', 'sku_id'])['price'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Inventory features\n",
    "    df['inventory_ratio'] = df['sales'] / (df['on_hand'] + 1)\n",
    "    \n",
    "    # Promotion features\n",
    "    df['promotions_flag'] = df['promotions_flag'].astype(int)\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "# Prepare features\n",
    "df_features, encoders = prepare_features(df)\n",
    "\n",
    "print(f\"Features prepared. Shape: {df_features.shape}\")\n",
    "print(f\"New columns: {[col for col in df_features.columns if col not in df.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    'store_id_encoded', 'sku_id_encoded', 'day_of_week', 'month', \n",
    "    'day_of_month', 'quarter', 'price', 'price_change', 'price_rolling_7',\n",
    "    'on_hand', 'inventory_ratio', 'promotions_flag',\n",
    "    'sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_lag_30',\n",
    "    'sales_rolling_7', 'sales_rolling_14', 'sales_rolling_30'\n",
    "]\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df_clean = df_features.dropna(subset=feature_columns + ['sales'])\n",
    "\n",
    "print(f\"Clean dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {len(df_features) - len(df_clean)} rows with missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['sales']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Split data (time-based split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training baseline model...\")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_pred = baseline_model.predict(X_train)\n",
    "test_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mae = mean_absolute_error(y_train, train_pred)\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "print(f\"\\nBaseline Model Performance:\")\n",
    "print(f\"Train MAE: {train_mae:.3f}\")\n",
    "print(f\"Test MAE: {test_mae:.3f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': baseline_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_samples_split': [10, 20],\n",
    "    'min_samples_leaf': [4, 8]\n",
    "}\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Use a subset for faster tuning\n",
    "sample_size = min(10000, len(X_train))\n",
    "sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_sample = X_train.iloc[sample_indices]\n",
    "y_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {-grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train optimized model\n",
    "optimized_model = grid_search.best_estimator_\n",
    "optimized_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "opt_train_pred = optimized_model.predict(X_train)\n",
    "opt_test_pred = optimized_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "opt_train_mae = mean_absolute_error(y_train, opt_train_pred)\n",
    "opt_test_mae = mean_absolute_error(y_test, opt_test_pred)\n",
    "opt_train_rmse = np.sqrt(mean_squared_error(y_train, opt_train_pred))\n",
    "opt_test_rmse = np.sqrt(mean_squared_error(y_test, opt_test_pred))\n",
    "\n",
    "print(f\"\\nOptimized Model Performance:\")\n",
    "print(f\"Train MAE: {opt_train_mae:.3f}\")\n",
    "print(f\"Test MAE: {opt_test_mae:.3f}\")\n",
    "print(f\"Train RMSE: {opt_train_rmse:.3f}\")\n",
    "print(f\"Test RMSE: {opt_test_rmse:.3f}\")\n",
    "\n",
    "print(f\"\\nImprovement over baseline:\")\n",
    "print(f\"MAE improvement: {((test_mae - opt_test_mae) / test_mae * 100):.1f}%\")\n",
    "print(f\"RMSE improvement: {((test_rmse - opt_test_rmse) / test_rmse * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Baseline model\n",
    "axes[0].scatter(y_test, test_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Sales')\n",
    "axes[0].set_ylabel('Predicted Sales')\n",
    "axes[0].set_title(f'Baseline Model (MAE: {test_mae:.3f})')\n",
    "\n",
    "# Optimized model\n",
    "axes[1].scatter(y_test, opt_test_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Sales')\n",
    "axes[1].set_ylabel('Predicted Sales')\n",
    "axes[1].set_title(f'Optimized Model (MAE: {opt_test_mae:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_test - opt_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(opt_test_pred, residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Sales')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residuals vs Predicted')\n",
    "\n",
    "# Residuals histogram\n",
    "axes[1].hist(residuals, bins=50, alpha=0.7)\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residuals Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residuals statistics:\")\n",
    "print(f\"Mean: {residuals.mean():.3f}\")\n",
    "print(f\"Std: {residuals.std():.3f}\")\n",
    "print(f\"Min: {residuals.min():.3f}\")\n",
    "print(f\"Max: {residuals.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by different segments\n",
    "test_data = df_clean.iloc[X_test.index].copy()\n",
    "test_data['predictions'] = opt_test_pred\n",
    "test_data['residuals'] = y_test - opt_test_pred\n",
    "test_data['abs_error'] = np.abs(test_data['residuals'])\n",
    "\n",
    "# Performance by store\n",
    "store_performance = test_data.groupby('store_id')['abs_error'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 stores with highest prediction error:\")\n",
    "print(store_performance.head(10))\n",
    "\n",
    "# Performance by promotion\n",
    "promo_performance = test_data.groupby('promotions_flag')['abs_error'].mean()\n",
    "print(f\"\\nPrediction error by promotion:\")\n",
    "print(f\"No promotion: {promo_performance[0]:.3f}\")\n",
    "print(f\"With promotion: {promo_performance[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Algorithm: Gradient Boosting Regressor\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"\")\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"- Test MAE: {opt_test_mae:.3f}\")\n",
    "print(f\"- Test RMSE: {opt_test_rmse:.3f}\")\n",
    "print(f\"- Improvement over baseline: {((test_mae - opt_test_mae) / test_mae * 100):.1f}%\")\n",
    "print(f\"\")\n",
    "print(f\"Key Insights:\")\n",
    "print(f\"- Most important features: {', '.join(feature_importance.head(3)['feature'].tolist())}\")\n",
    "print(f\"- Model performs better on non-promotional periods\")\n",
    "print(f\"- Some stores are harder to predict than others\")\n",
    "print(f\"\")\n",
    "print(f\"Recommendations:\")\n",
    "print(f\"- Consider store-specific models for high-error stores\")\n",
    "print(f\"- Improve promotion effect modeling\")\n",
    "print(f\"- Collect additional features (weather, events, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "1. **Production Implementation**: Use the `ml/scripts/train.py` script for production training\n",
    "2. **Model Monitoring**: Set up automated retraining based on performance degradation\n",
    "3. **Feature Enhancement**: Add external data sources (weather, holidays, events)\n",
    "4. **Advanced Models**: Experiment with time series models (ARIMA, Prophet) or deep learning approaches\n",
    "5. **Business Integration**: Implement inventory optimization based on predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}